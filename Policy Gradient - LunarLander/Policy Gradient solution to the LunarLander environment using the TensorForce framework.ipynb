{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463df08b",
   "metadata": {},
   "source": [
    "# *Policy Gradient solution to the LunarLander environment using the TensorForce framework*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f51b85",
   "metadata": {},
   "source": [
    "___This tutorial will:___\n",
    "\n",
    "* Briefly introduce the Policy Gradient/REINFORCE algorithm;\n",
    "* Introduce the reader to the LunarLander environment from the Open AI Gym toolkit;\n",
    "* Walk the reader through a solution to said environment using the TensorForce framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c7243",
   "metadata": {},
   "source": [
    "## An introduction to Policy Gradient methods and the REINFORCE algorithm\n",
    "\n",
    "The REINFORCE algorithm belongs to the family of Policy Gradient methods for Reinforcement Learning, whose goal is to directly model and optimize the *policy*, which is the strategy that the agent follows in order to reach a goal. In Policy Gradient methods, the policy is usually modeled with a parameterized in respect to $\\theta$:\n",
    "$$ \\pi_\\theta = (a_t|s_t) $$\n",
    "Which represents the probability, dependent on the parameters $\\theta$, that action $a_t$ is chosen, given the state $s_t$ at timestep $t$.\n",
    "A Policy Gradient method's idea, then, is to pick an action at each timestep following a probabilistic approach. Once an episode ends, the parameters of the distribution are adjusted according to the observed *reward*.\n",
    "The REINFORCE algorithm does just that; compared to other Policy Gradient algorithms, this might be considered rather rudimentary, but this makes it very straightforward, since it's almost a direct implementation of the concepts discussed before. The algorithm works as follows:\n",
    "\n",
    "1. _Initialize a random policy (almost always a Neural Network that takes states as inputs and generates actions as outputs);_\n",
    "2. _Use the policy to play N steps of the game and record action probabilities from policy, reward-from environment and action from the agent;_\n",
    "3. _Calculate the discounted reward for each step by backpropagation;_\n",
    "4. _Calculate expected reward R;_\n",
    "5. _Adjust weights of Policy (back-propagate error in NN) to increase R;_\n",
    "6. _Repeat from step 2._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532f4f1",
   "metadata": {},
   "source": [
    "## LunarLander from Open AI Gym: The Environment\n",
    "\n",
    "The environment that the agent will have to work in for this tutorial is the LunarLander environment from the Open AI Gym toolkit. It is a not-so-trivial environment, and it's available in both a continuous and discrete action space version. In this tutorial, the discrete version will be tackled. The environment consists of a lander which has to safely land on the moon, preferably between two flags that represent the landing pad.\n",
    "\n",
    "\n",
    "<div><img src=\"3.gif\", width=750px, height=500px /></div>\n",
    "\n",
    "Formally, the environment works as follows: the landing pad is at coordinate (0,0), and the lander's coordinates can be found as the first two values in the agent's current registered state. The agent has four actions at its disposal:\n",
    "1. Do nothing;\n",
    "2. Fire left orientation engine;\n",
    "3. Fire main engine;\n",
    "4. Fire right orientation engine.\n",
    "\n",
    "The more the lander strays from the landing pad's coordinates, the more points it loses. An episode ends when the lander either lands safely or crashes, which grants +100 and -100 additional points respectively. Firing the main engine is -0.3 points each frame, and for each lander leg contact on the surface of the \"moon\", the agent gets +10 points. Fuel is infinite, and the environment is considered completed once the agent reaches 200 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970404c7",
   "metadata": {},
   "source": [
    "## Environment solution in TensorForce\n",
    "\n",
    "Now it's time to practically tackle the environment solution. This will be done in the TensorForce framework for Reinforcement Learning, for which an introduction is provided in this same repository. To achieve the goal of training and evaluating the agent, a modified version of the act-experience-update script will be used, which simply separates the training and evaluation routines into two different functions, mainly for the sake of clarity.\n",
    "\n",
    "### Step 1: The training function\n",
    "\n",
    "The first step into defining the training function is to initialize the necessary lists, which are those that will be used to store the episode information that will be fed to the agent at the end of each episode. These are, in practice, five different list, which store the state information, terminal information of each state, information on the agent's internals, action information and reward information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, num_episodes):\n",
    "    tr_sum = 0.0\n",
    "    for episode in range(num_episodes):\n",
    "        ep_states = list()\n",
    "        ep_actions = list()\n",
    "        ep_internals = list()\n",
    "        ep_terminal = list()\n",
    "        ep_rewards = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91793cee",
   "metadata": {},
   "source": [
    "After that, it's time to initialize the states and the agent internals, as well as asserting that the first state is *not* terminal, which is done via the following lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce034dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "        states = environment.reset()\n",
    "        internals = agent.initial_internals()\n",
    "        terminal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4ac61",
   "metadata": {},
   "source": [
    "Now to the core of the training routine. This consists of a loop in which first the current state and internals information is stored in the appropriate lists, then a call to the *agent.act()* function is issued, which returns the chosen action and the new internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "        while not terminal:\n",
    "            ep_states.append(states)\n",
    "            ep_internals.append(internals)\n",
    "            actions, internals = agent.act(states=states, internals=internals, independent=True, deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6e266",
   "metadata": {},
   "source": [
    "The action information is then stored in the right list before calling the *environment.execute()* function, which takes the returned action as input and produces the new state, whether it's terminal or not, and the observed reward at the current timestep. Finally, the reward is added to the previously observed reward, and the look goes on until a terminal state is reached, after which the agent is fed the new information via the *agent.experience()* and *agent.update()* functions. All of this is repeated as many times as specified in the training function's *num_episodes* attribute, and finally the overall observed reward is divided by the total number of episodes and produced as output for the main routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "            ep_actions.append(actions)\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "            ep_terminal.append(terminal)\n",
    "            ep_rewards.append(reward)\n",
    "            tr_sum += reward\n",
    "        agent.experience(states=ep_states, actions=ep_actions, terminal=ep_terminal, reward=ep_rewards,\n",
    "                         internals=ep_internals,)\n",
    "        agent.update()\n",
    "    tr_sum = tr_sum/num_episodes\n",
    "    return tr_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4b929",
   "metadata": {},
   "source": [
    "### Step 2: the evaluation function\n",
    "\n",
    "The evaluation routine, in this solution with TensorForce, doesn't differ much from the training one. This is the section of code that handles, as the name suggests, the evaluation of the agent, making sure to test the agent on what it has learned in a given number of training episodes. The major difference between this section and the training routine is that no list is initialized, since clearly no new information needs to be given to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, environment, num_episodes):\n",
    "    ev_sum = 0.0\n",
    "    for t in range(num_episodes):\n",
    "        states = environment.reset()\n",
    "        internals = agent.initial_internals()\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            actions, internals = agent.act(states=states, internals=internals, independent=True)\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "            ev_sum += reward\n",
    "    ev_sum = ev_sum/num_episodes\n",
    "    return ev_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9368a2c4",
   "metadata": {},
   "source": [
    "Another difference between the two is that, in this case, the *deterministic* parameter does not appear in the agent.act call, or, to be more exact, it is left at its default <font color=green>**True**</font> value. This is because a true value of the deterministic parameter forces the agent to choose the next action deterministically, which is to say without exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768de61",
   "metadata": {},
   "source": [
    "### Step 3: Parameter inizialitation\n",
    "\n",
    "After defining the training and evaluation routines, it's time to initialize the parameters that will be used in the main one, along with creating the correct instances of the *agent* and the *environment* that will be used. First off, the environment must be initialized, and this particular instance of the *LunarLander* environment will have a number of 450 maximum timesteps per episode. This is no randomly chosen number: a lower value, in fact, will mean that the agent will most likely fail more often, since it would not have had enough time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 450\n",
    "my_env = tf.Environment.create(environment='gym', level='LunarLander-v2', max_episode_timesteps=max_time, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cb7a4",
   "metadata": {},
   "source": [
    "Before creating a new agent, the Policy Network must be initialized, just like in the first step of the algorithm discussed earlier in this tutorial. This is a Neural Network that is used to represent the *policy* that must be optimized by the agent and consists of three *Dense* layers, the first two having a size of 10 and a *relu* activation function and the last layer with size equal to the number of possible actions that can be taken by the agent and a *softmax* activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb32a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSpace = my_env.actions()\n",
    "n_actions = actionSpace.get('num_values')\n",
    "network_spec = [dict(type='dense', size=10, activation='relu'),\n",
    "                dict(type='dense', size=10, activation='relu'),\n",
    "                dict(type='dense', size=n_actions, activation='softmax')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca70e3",
   "metadata": {},
   "source": [
    "After this, it is finally time to create the agent. Agent creation is entirely handled by TensorForce aside from providing the correct parameter. In this case, the agent created should be a *VanillaPolicyGradient* agent, which is defined by the identifiers *vpg* or *reinforce* in TensorForce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = tf.Agent.create(agent='reinforce', environment=my_env, batch_size=8, network=network_spec, memory=2000000,\n",
    "                           learning_rate=5e-4, discount=0.99, exploration=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303842b3",
   "metadata": {},
   "source": [
    "Note that the learning rate and exploration values are, once again, not randomly chosen: given the parameters specified in the previous steps, a lower learning rate would result in the agent taking enormous amounts of time in order to learn how to solve the environment and cause overfitting issues, while a higher learning rate would lead the agent to learn \"too fast\", causing underfitting issues and often leading it to not finding a correct solution. As far as the exploration goes, it is necessary in order to prevent the agent into getting stuck in a local maximum; a lower value would fail to prevent such a thing to happen, but a much higher value would lead the agent into exploring too much, preventing it from finding a consistent solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e963f",
   "metadata": {},
   "source": [
    "### Step 4: the main routine\n",
    "\n",
    "Finally, the main routine will be discussed along with some final results from its execution. This routine consists in a loop in which the training and evaluation functions are called, thus, every iteration of said loop is considered an *epoch*. For this tutorial, the training and evaluation routines will be repeated for 150 epochs, with 150 training steps and 50 evaluation steps each. At the end of all of the training steps, for each epoch, the resulting average reward is appended in a list, same with the evaluation average reward, and will later be used for plotting the training and evaluation rewards over the total number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce335fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.reset()\n",
    "train_rewards = []\n",
    "rewards = []\n",
    "for i in range(150):\n",
    "    # Training\n",
    "    tr = train(my_agent, my_env, 150)\n",
    "    print(\"Training: \", tr)\n",
    "    print('Cleared training ', i)\n",
    "    train_rewards.append(tr)\n",
    "    # Evaluation\n",
    "    ev = evaluate(my_agent, my_env, 50)\n",
    "    print(str(ev))\n",
    "    print('Cleared epoch ', i)\n",
    "    rewards.append(ev)\n",
    "\n",
    "my_env.close()\n",
    "my_agent.close()\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg_rewards_eval')\n",
    "plt.show()\n",
    "plt.plot(train_rewards)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg_rewards_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b61bd4",
   "metadata": {},
   "source": [
    "Below are provided two plots returned by this routine's application. Note that these do not reach the maximum amount of points, but only because the number of epochs used is slightly lower than needed, which can easily be adjusted in the main routine. Nevertheless, it can be noted that the agent starts from extremely negative average reward values and slowly begins to learn more and more how to land the lander on the moon's surface.\n",
    "\n",
    "\n",
    "<div><img src=\"LunarLanderTrainDEF.png\", width=450px, height=450px, align=\"left\" /></div>\n",
    "<div><img src=\"LunarLanderEvalDEF.png\", width=450px, height=450px, align=\"right\" /></div>"
   ]
  },
  
