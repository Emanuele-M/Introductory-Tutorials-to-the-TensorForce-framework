{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07abb69c",
   "metadata": {},
   "source": [
    "# *Policy Gradient agent implementation in Tensorforce*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f950b",
   "metadata": {},
   "source": [
    "___This tutorial will:___\n",
    "* briefly introduce the Policy Gradient algorithm;\n",
    "* walk the reader through the implementation of a simple Policy Gradient agent <br>  using the Tensorforce framework  and its usage to solve a basic OpenAi Gym problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f0702",
   "metadata": {},
   "source": [
    "# A brief introduction to the TensorForce framework\n",
    "\n",
    "In this tutorial, the implementation of the Policy Gradient agent will be entirely realized through the usage of the RL framework Tensorforce. TensorForce is an open-source framework which provides high level, user friendly interfaces and tools to operate in the field of deep RL. It's built on top of Google's TensorFlow framework and requires Python 3 to funcion. An important aspect which TensorForce focuses on is the general applicability of every feature implementation provided, which translates to algorithms made to be agnostic of the structure and type of the inputs and the outputs related to specific scenarios. This framework can be used to implement a wide variety of highly customizable agents and environments through a contained number of functions, from simple Policy Gradient agents to DQNs to Acto. For instance, in order to create a constant agent, the *Agent.create* function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4faa63f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-1-aaaf9db95e28>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-aaaf9db95e28>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    constant_agent = tensorforce.Agent.create(agent='constant', ...)\u001b[0m\n\u001b[1;37m                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "constant_agent = tensorforce.Agent.create(agent='constant', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ae048",
   "metadata": {},
   "source": [
    "The same can be done when it comes to environment creation, whether a custom environment or one from the OpenAI Gym is used. To create an instance of the MountainCar environment from Gym, for example, the *Environment.create* function must be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab0141b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-2-5d78d118354f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-5d78d118354f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    mountain_car = tensorforce.Environment.create(environment='gym', level='MountainCar', ...)\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "mountain_car = tensorforce.Environment.create(environment='gym', level='MountainCar', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc9d35",
   "metadata": {},
   "source": [
    "This framework also provides an execution utility equipped to handle both the training and evaluation of agents: the Runner utility, which offers a wide range of configuration options and can be used to train a single agent as well as multiple agents at the same time. A more detailed introduction to the framework and its tools can be found on TensorForce's [official website](https://tensorforce.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe8828",
   "metadata": {},
   "source": [
    "# Policy Gradient: the idea\n",
    "\n",
    "The term Policy Gradient refers to a family of RL algorithms based on the optimization of a *policy*: $$ \\pi_\\theta = (a_t|s_t) $$\n",
    "Which is the probability, dependent on the paramenters $\\theta$, that action $a_t$ is chosen given the state $s_t$ at time $t$.\n",
    "The basic idea of a Policy Gradient method consists in using probabilistic action choice to select an action to execute at each timestep. Once an episode is over, the parameters are adjusted according to the value of the overall *reward* (or *return*).\n",
    "In general, the policy is represented through a neural network known as the *Policy Network*. \n",
    "Since an action is picked at each timestep, it's then possible to put all of the timesteps together to define a *trajectory*:\n",
    "$$\\tau = (\\overline{a}, \\overline{s})$$\n",
    "Where $\\overline{a} = a_0, a_1, a_2...$ is a sequence of actions and $\\overline{s} = s_1, s_2...$ is a sequence of states ($s_0$ is fixed), and subsequently calculate the following probability: $$P_{\\theta}(\\tau) = \\prod_t{P(s_{t+1}|s_t,a_t)\\tau_{\\theta}(a_t|s_t)}$$\n",
    "Which allows to find the expected overall reward: $$\\overline{R} = E[R] = \\sum_{\\tau}P_{\\theta}(\\tau)R(\\tau)$$\n",
    "The formula right above shows that the expected overall reward is given by the sum over all trajectories of the corresponding reward multiplied by the probability of following that trajectory.<br>\n",
    "The goal is to maximize this reward, and that's where the *Gradient* part of Policy Gradient comes into play. In order to find the values for each parameter in $\\theta$ which maximize the reward, it's necessary to calculate the *gradient* of the expected reward over the parameters, which results in: $$\\dfrac{\\partial \\overline{R}}{\\partial \\theta} = \\sum_t{E[R\\dfrac{\\partial ln\\tau_\\theta(a_t|s_t)}{\\partial \\theta}]}$$\n",
    "This formula not only gives a method for the mathematical computation of this gradient, but also shows how it's only dependent on the policy, not on the environment this method is applied in, which means the Policy Gradient method is ***Model Free***.\n",
    "The parameters are then updated following the formula: $$\\theta_{new} = \\theta_{old} + \\alpha\\dfrac{\\partial \\overline{R}}{\\partial \\theta}$$\n",
    "Where $\\alpha$ is the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3525271",
   "metadata": {},
   "source": [
    "# OpenAi Gym: The environment\n",
    "\n",
    "For the sake of this tutorial, the Policy Gradient agent will be tasked to solve the *CartPole* environment from the OpenAI gym toolkit. This is the most basic environment among those provided by gym, and is to Deep Learning what \"*Hello World*\" is to general programming. The problem is extremely straightforward: the agent must prevent a pole precariously balanced on a cart from falling by any means necessary. \n",
    "<div><img src=\"78819170-cb8f0780-79a3-11ea-8ad6-069968da4d14.gif\", width=250px, height=250px /></div>\n",
    "The cart moves on a frictionless surface, and it's controlled by applying a force in order to move it to the left or to the right. For each timestep the pole stays upright, a reward of +1 point is provided to the agent, and an episode terminates either by reaching the timestep limit, which qualifies as a success, by moving 2.4 units away from the center of the platform or if the pole is more than 15 degrees from vertical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5640c2",
   "metadata": {},
   "source": [
    "# Solution implementation: the TensorForce framework\n",
    "\n",
    "## Step 1: The training function\n",
    "\n",
    "In order to properly train and evaluate the agent, in this tutorial will be exploited a slightly modified version of TensorForce's act-experience-update interface: the agent is trained for a certain number of episodes, where at the end of each one the *agent.experience* and *agent.update* TensorForce functions will be called in order to, respectively, feed the agent the episode returns and updating the agent's parameters (which correspond to the $\\theta$ parameters discussed in the Policy Gradient paragraph). In this version of the interface, the training and evaluation steps are divided into two functions for the sake of clarity.\n",
    "    \n",
    "* ### Step 1.1: initializing the variables and lists of returns\n",
    "    \n",
    "  The first step consists in initializing five lists to store the different kinds of returns provided by the functions used during training, which will be covered shortly. After this, three variables have to be initialized: one to keep track of the current *state*, which is initialized by resetting the environment, one to store the current agent internals and a boolean variable which is used to determine whether a state is terminal or not.\n",
    "\n",
    "import tensorforce as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(agent, environment, num_episodes):\n",
    "    tr_sum = 0.0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58880da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, num_episodes):   \n",
    "    for episode in range(num_episodes):\n",
    "        ep_states = list()\n",
    "        ep_actions = list()\n",
    "        ep_internals = list()\n",
    "        ep_terminal = list()\n",
    "        ep_rewards = list()\n",
    "        states = environment.reset()\n",
    "        internals=agent.initial_internals()\n",
    "        terminal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8838c",
   "metadata": {},
   "source": [
    "* ### Step 1.2: The training loop\n",
    "    \n",
    "    Once the variables and lists are initialized, the actual training phase is tackled. The main body consists in a loop that goes on until a terminal state is reached. In this loop can be found two TensorForce functions:\n",
    "        \n",
    "    * *agent.act*, which takes the current state and the agent internals as input and outputs the chosen action to take as well as the new internals;\n",
    "    * *envirnoment.execute*, which takes the action provided by the previous function as input and outputs the reward at the current timestep, the new state and the indication of whether such state is terminal.\n",
    "        \n",
    "    the outputs are appended to the corresponding lists and, when a terminal state is reached, the *agent.experience* and *agent.update* functions are called. Finally the function returns the mean reward over the total number of training episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ddca548",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-6c55d0b08b26>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-6c55d0b08b26>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    tr_sum = tr_sum/num_episodes\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "        while not terminal:\n",
    "            ep_states.append(states)\n",
    "            ep_internals.append(internals)\n",
    "            actions, internals = agent.act(states=states, internals=internals, independent=True, deterministic=False)\n",
    "            ep_actions.append(actions)\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "            ep_terminal.append(terminal)\n",
    "            ep_rewards.append(reward)\n",
    "            tr_sum += reward\n",
    "        agent.experience(states=ep_states, actions=ep_actions, terminal=ep_terminal, reward=ep_rewards,\n",
    "                            internals=ep_internals,)\n",
    "        agent.update()\n",
    "    tr_sum = tr_sum/num_episodes\n",
    "    return tr_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054e773",
   "metadata": {},
   "source": [
    "## Step 2: The evaluation function\n",
    "\n",
    "After defining the training function, it's time to create an evaluation function, which will have the agent execute the task in the environment for a certain number of episodes without feeding it any returns. Apart from this major difference, the implementation of this function is very similar to the previous one, although without the lists used to store the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf76d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, environment, num_episodes):\n",
    "    ev_sum = 0.0\n",
    "    for t in range(num_episodes):\n",
    "        states = environment.reset()\n",
    "        internals = agent.initial_internals()\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            actions, internals = agent.act(states=states, internals=internals, independent=True)\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "            ev_sum += reward\n",
    "    ev_sum = ev_sum/num_episodes\n",
    "    return ev_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8975c",
   "metadata": {},
   "source": [
    "One thing worth noting is that, in this case, the argument deterministic=<font color=green>**False**</font> is not given as input to the agent.act function. This is because in the evaluation phase the agent needs to select actions deterministically, without exploration, and <font color=green>**True**</font> is the default value for the deterministic parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871a0c6",
   "metadata": {},
   "source": [
    "## Step 3: Initializing the parametes\n",
    "\n",
    "Once the training and evaluation routines are defined, the next step is initializing the parameters that will allow the execution of the main routine. Moreover, in this step both the environment and the agent are initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 100\n",
    "my_env = tf.Environment.create(environment='gym', level='CartPole', max_episode_timesteps=max_time, visualize=True)\n",
    "actionSpace = my_env.actions()\n",
    "n_actions=actionSpace.get('num_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df30af",
   "metadata": {},
   "source": [
    "Interestingly, there is no need to import the OpenAI Gym toolkit in order to initialize an environment as an instance of CartPole, the correct usage of which is handled directly by the TensorForce function.\n",
    "\n",
    "* ### Step 3.1: The Policy Network\n",
    "\n",
    "    At this point, the policy network, which was briefly introduced in the first paragraph of this tutorial, must also be instantiated. In this case, it consists of a simple neural network with three layers, two dense layers with a size of 64 and a *relu* activation and another dense layer, where this time the size corresponds to the dimention of the action space of the CartPole environment and with a *softmax* activation. This network is then used as the policy of the policy gradient agent. The creation of the agent is entirely handeled by TensorForce aside from providing the correct values for the input parameters. When calling the funcion *agent.create*, an identifier must be provided so that it can create the correct type of agent, which in this case will be a \"vpg\", Vanilla Policy Gradient, or \"reinforce\" agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_spec = [dict(type='dense', size=64, activation='relu'),\n",
    "                dict(type='dense', size=64, activation='relu'),\n",
    "                dict(type='dense', size=n_actions, activation='softmax')]\n",
    "\n",
    "my_agent = tf.Agent.create(agent='reinforce', environment=my_env, batch_size=64, network=network_spec,\n",
    "                           memory=10000, learning_rate=5e-4, discount=0.99, baseline='auto',\n",
    "                           baseline_optimizer=dict(optimizer='adam', learning_rate=5e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea39cc2",
   "metadata": {},
   "source": [
    "The values for the learning rate, batch size, memoy and discount parameters can be modified as the reader sees fit, while keeping in mind the general rules of thumb regarding said values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129e243",
   "metadata": {},
   "source": [
    "## Step 4: The main routine\n",
    "\n",
    "Now everything is ready to realize the main program, which will allow the training and evaluation of the agent in the environment for an arbitrary number of epochs. In this tutorial, the agent will do so for 100 epochs, storing the total training and evaluation rewars per epoch in order to subsequently plot them to help visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab4d9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-41e0d92c8792>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_agent' is not defined"
     ]
    }
   ],
   "source": [
    "my_agent.reset()\n",
    "train_rewards = []\n",
    "rewards = []\n",
    "for i in range(100):\n",
    "    # Training\n",
    "    training = train(my_agent, my_env, 10)\n",
    "    train_rewards.append(training)\n",
    "    # Evaluation\n",
    "    sum_reward = evaluate(my_agent, my_env, 100)\n",
    "    print(str(sum_reward))\n",
    "    print('Cleared epoch ', i)\n",
    "    rewards.append(sum_reward)\n",
    "\n",
    "my_env.close()\n",
    "my_agent.close()\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg_rewards_eval')\n",
    "plt.show()\n",
    "plt.plot(train_rewards)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('avg_rewards_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18178f",
   "metadata": {},
   "source": [
    "Below are given two plots corresponding to an example of execution of the program. It can be noted how the agent never manages to reach the maximum amount of points during training, while still showing an upward trend as expected, while during the evaluation it reaches the maximum in a very minimal amount of epochs.\n",
    "\n",
    "<div><img src=\"VPG_train_10train.png\", width=450px, height=450px, align=\"left\" /></div>\n",
    "<div><img src=\"VPG_eval_10train.png\", width=450px, height=450px, align=\"right\" /></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
