{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5816077",
   "metadata": {},
   "source": [
    "# *Introduction to the TensorForce framework for Deep Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f02a8",
   "metadata": {},
   "source": [
    "TensorForce is one of the most recent additions to the family of Deep Reinforcement Learning frameworks at the disposal of computer engineers. Built on top of Google's TensorFlow, it is an open-source framework which focuses on high-level, flexible library design and straightforward usability for practical and research oriented applications. This is achieved through various devices: for example, the algorithms provided are agnostic of type and structure of inputs and outputs, making them versatile and not bound to be used on a restricted set of environments and situations.\n",
    "This tutorial will provide an introduction to the essential functions provided by this framework, which offers customization options and features for:\n",
    "\n",
    "* Agents;\n",
    "* Environments;\n",
    "* Modules;\n",
    "* Execution;\n",
    "\n",
    "Note that in this tutorial only the basics of each function and its usages will be covered. A more detailed explaination of the parameters used can be found on [TensorForce's official site](https://tensorforce.readthedocs.io/en/latest/index.html#)\n",
    "\n",
    "## The TensorForce Agents\n",
    "\n",
    "TensorForce allows to create and manage a large number of agent types, ranging from the Constant Agent, to the Policy Gradient Agent, to the Deep Q-Network and the A2C Agents, as well as a *TensorForce Agent*, a highly customizable agent. Whilst the different kinds of possible agents provided are numeous, their basic implementation is pretty much the same: the bulk of it is, in fact, entirely managed by TensorForce, leaving to the developer the sole requirement of defining the correct agent hyperparameters needed for each use case. The main functions used for the creation and management of agents in this framework are:\n",
    "\n",
    "* **tensorforce.Agent.create(...)**, which, as the name suggests, creates an instance of an agent. The different kinds of agents are distinguished (and thus can be instantiated) by an identifier. For example, if the function must be used to create a random agent, the parameter for the agent type would have to be *agent='random'*. \n",
    "\n",
    "* **tensorforce.Agent.act(...)** is another core function for a TensorForce agent, being the one that allows it to interact with the environment. Taking the current state and some internal state information, it returns the action that has been picked to be executed in the current timestep.\n",
    "\n",
    "* **tensorforce.Agent.observe(...)** is used to observe the reward resulting from the execution of the selected action and verify if a terminal state is reached. It must always be preceded by a call to the *act()* function and is part of TensorForce's *act-observe* script, one of the possible ways to implement agent training in TensorForce.\n",
    "\n",
    "* **tensorforce.Agent.experience(...)** and **tensorforce.Agent.update()** are two functions which can be used as an alternative to the *act-observe* script, constituting the *act-experience-update* script for agent training. the *experience()* function feeds the agent new information about reached states, whether these were terminal or not, the actions taken to reach said states, the reward resulting from their executions and new internal states information, whilst the *update()* function proceeds to update the agent.\n",
    "\n",
    "* **tensorforce.Agent.load()** and **tensorforce.Agent.save()**, finally, are functions that can be used in order to save a specific agent configuration to any given directory or load a previously saved configuration. They can be very useful if a particular agent is needed in multiple projects.\n",
    "\n",
    "\n",
    "## Environments in TensorForce\n",
    "\n",
    "When it comes to environment creation and usage, similarly to what was described in the previous section for agents, TensorForce provides various assets to realize highly customizable instances of RL environments. Along with the ability to create a completely custom environment, it also allows access to various premade ones, such as the OpenAI Gym environments and the PyGame Learning Environment. Much like with agents, the main functions available to manage environments in the TensorForce framework are pretty straightforward:\n",
    "\n",
    "* **tensorforce.Environment.create(...)** is the function needed to create an instance of an environment from a specification, which can range from a JSON file, a specification key, a configuration dictionary and more.\n",
    "\n",
    "* **tensorforce.Environment.states(), tensorforce.Environment.actions()** and **tensorforce.Environment.max_episode_timesteps** can be used in order to get, respectively, the environment's possible states as a dictionary containing the states' data type, the shape, the number of states and their min/max values, a similar dictionary for the actions that can be taken in the environment and the maximum number of timesteps per episode.\n",
    "\n",
    "* **tensorforce.Environment.execute(*actions*)** which, given an action (likely the return of the *Agent.act()* function), returns the next state reached, whether it's a terminal state and the current timestep's observed reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37203c02",
   "metadata": {},
   "source": [
    "## Module Customization\n",
    "\n",
    "The presence of customizable modules in TensorForce is a pretty unique characteristic of this environment that differenciates it from those with similar goals of abstracion in RL: these allow, in fact, to easily change model specifics that are *within* the agent, affecting its policy and allowing the developer to modify it with a small function. There is a wide variety of modules for which such an abstraction is provided:\n",
    "\n",
    "* **Distributions**: the probability distributions of the policy's parameters, which can be set to *Gaussian*, *Categorical*, *Bernoulli* or *Beta*.\n",
    "* **Layers**: The different kinds of layers that can be used to build the policy network. Every type of layer can be specified through their identifier and a plethora of layer types can be managed, from Dense layers to Convolutional layers to Keras layers.\n",
    "* **Memories**: Allows the user to choose between a *Replay* memory and a *Recent* one, where the first randomly retrieves agent experiences and the second always retrieves the most recent experience.\n",
    "* **Networks**: For networks, TensorForce provides three main options: the *Auto network*, which is an automatically configured network based on input types and shapes, the *layered network* and the *Keras Network*, a wrapper that allows the creation of a network specified as a Keras model.\n",
    "* **Objectives**: Policy objectives, can be set to *PolicyGradient*, *Value*, *DeterministicPolicyGradient* or *Plus*.\n",
    "* **Optimizers**: TensorForce permits to manage layer optimizers not only giving the option of specifying which optimizer to use, but also how to appyly it. For example, the *DoubleCheckStep* update modifier checks whether the update given by the applyed optimizer has decreased the loss, and if not, reverses it.\n",
    "* **Parameters**: Parameter specification, which can be applied to agent/module arguments and values within the architecture (for example, the learning rate). Some notable configurations are *Constant*, *Linear, *Decaying*, *Random* and *Exponential*\n",
    "* **Policies**: In TensorForce, it's possible to easily modify the type of policy and its configuration within a model. The possible types provided are *ParametrizedActionValue*, *ParametrizedDistributions*, *ParametrizedStateValue*, *ParametrizedValuePolicy*.\n",
    "* **Preprocessing**: Reward preprocessing, which can be set as *Clipping*, *Image*, *LinearNormalization*, *InstanceNormalization* and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d521bb",
   "metadata": {},
   "source": [
    "## Execution Routine: the Runner utility\n",
    "\n",
    "The various customizable elements that have been discussed until this point were mostly related to the creation and specification of agents and environments. But TensorForce provides a very useful tool to facilitate and abstract the execution process: the **Runner** utility, whose purpose is to correctly and completely handle the training and/or the evaluation of an agent in a given environment. The runner is instantiated like any object, taking as parameters the agent, the environment and other useful information, like how many parallel environment instances it has to run. Interestingly, both the agent and the environment can be created directly when instantiating the runner, without using the *Agent.create()* and *Environment.create()* functions. This is because, if created this way, the runner will also handle the termination of both agent and environment, which should alternatively be done separately. The runner utility then provides one single funtion: **Runner.run(...)**, which runs the experiment according to the parameters given as input. For example, if Runner.run() is called and the *evaluation* parameter is set to *False*, the runner will run a training experiment, whilst if said parameter is set to *True*, it will run in evaluation mode. All the progress can be followed through a tqdm progress bar, which reports the cumulative episode return, the timesteps per episode, seconds per episode and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d140daf",
   "metadata": {},
   "source": [
    "## Initialization, training and evaluation of the agent in practice\n",
    "\n",
    "Now that all of the different features provided by the TensorForce framework have been introduced, it is time to use them together in order to initialize, train and evaluate an agent in an environment.\n",
    "\n",
    "* ### Step 1: initializing the Environment\n",
    "    \n",
    "    The first thing to do is initializing an environment. As said before, the environment in question can be an entirely custom one or a premade one provided by one of the available toolkits. For the sake of this example, an instance of the CartPole environment from the Open AI Gym will be initialized with a maximum amount of timesteps per episode of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment='gym', level='CartPole', max_episode_timesteps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e641e7",
   "metadata": {},
   "source": [
    "* ### Step 2: initializing the Agent\n",
    "\n",
    "    After this first step, the agent should be initialized. In this case, a simple instance of a TensorForce agent with a Policy Gradient objective will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe033e1",
   "metadata": {},
   "source": [
    "* ### Step 2: initializing the Agent\n",
    "\n",
    "    After this first step, the agent should be initialized. In this case, a simple instance of a TensorForce agent with a Policy Gradient objective will be used.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3883c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=environment, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129cdaa8",
   "metadata": {},
   "source": [
    "   One thing to note about this example is that the environment created in the previous step is passed to the *Agent.create()* function as a parameter. This is because, in order to function, the agent needs to access the *states* and *actions* parameters of the environment it's bound to work with; these could be specified separately, but it's always suggested to instead specify them implicitly through the *environment* parameter, since TensorForce is guaranteed to provide the correct values this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8783",
   "metadata": {},
   "source": [
    "* ### Step 3: Initializing the Runner, training and evaluation\n",
    "\n",
    "    Finally, the runner is initialized and it can thus be used in order to both train and evaluate the agent. As far as the first part goes, it's pretty straightforward: the Runner is initialized by passing as parameters the agent and the environment to the Runner and specifying the max_episode_timesteps if not already specified in the environment creation step, which is the case for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb79fe",
   "metadata": {},
   "source": [
    "Now to the training and the evaluation of the agent. These two steps correspond to two separate executions of the *Runner.run()* function, one of which, the second, will be in evaluation mode. This is the simplest usage of the runner utility, but it can also be run with more detailed specifications in different use cases. For this example, the agent will be trained in the CartPole environment for 200 episodes, and then evaluated for 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78901e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run(num_episodes=200)\n",
    "runner.run(num_episodes=100, evaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
